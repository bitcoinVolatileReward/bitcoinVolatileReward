{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7cc87b8-4f75-4acb-87b9-4430204012a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining smaple points for time-fee equation ...\n",
      "Number of steps: 50000\n",
      "Number of steps: 100000\n",
      "************************** Linear regression****************************\n",
      "Time-fee equation is: f(t) = 0.072180 * t + 1.132566\n",
      "************************** Curve regression****************************\n",
      "Time-fee equation is: y(x) = 2.492318 * ln( t^(1.327729) + 63.921420) + -9.213203\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining the normal adversarial block reward per minute while mining honestly ...\n",
      "Normal adversarial block reward per minute 0.08327665429732464\n",
      "Obtaining the average fee ratio of 30-minute and 20-minute blocks to a 10-minute block ...\n",
      "Average 10-min block fee: 1.8547298889436368\n",
      "Average 20-min block fee: 2.7420216053781896\n",
      "Average 30-min block fee: 3.488659830648281\n",
      "Average ratio of a 20-min block to a 10-min block: 1.4783940355540994\n",
      "Average ratio of a 30-min block to a 10-min block: 1.8809530441304585\n",
      "Obtaining an MDP-based lower bound for selfish mining profitability before difficulty adjustment ...\n",
      "numOfStates: 314880\n",
      "processing state: 0\n",
      "processing state: 100000\n",
      "processing state: 200000\n",
      "processing state: 300000\n",
      "A lower bound for selfish mining profitability ratio before difficulty adjustment 1.2621925074879057\n",
      "A3C trining is starting ...\n",
      "A3C_args: {'lr': 1e-06, 'gamma': 0.99, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 50, 'noise_std': 0.2, 'reward_scaling_coefficient': 0.1, 'N_nodes_per_layer': 256, 'seed': 1, 'num_processes_train': 30, 'num_processes_test': 2, 'num_steps': 10, 'testing_episode_length': 100000, 'training_print_length': 1000, 'max_episode_length': 1000000, 'env_name': 'Blockchain', 'long_range_testing': False, 'testing_index': 0, 'no_shared': False}\n",
      "BTC_args {'adversarial_ratio': 0.45, 'rational_ratio': 0, 'connectivity': 1, 'max_fork': 20, 'len_abort': 1, 'mining_time': 10, 'block_fixed_reward': 0, 'max_block_size': 1, 'attack_type': 1, 'noise': False, 'epsilon': 0, 'Diff_adjusted': False, 'state_length': 455, 'n_action': 7}\n",
      "Mempool_args {'start_date_str': '18/12/2023_23_00_00', 'end_date_str': '18/12/2023_23_59_59', 'N_memPool_section': 10, 'sat_per_byte_range_length': 50, 'N_steps_honest_mining': 100000, 'Base_sat_per_byte_range': 80, 'coefficient_mempool_reward': [[0.14820472025394815, 0.7282113495520344, 5.643656907094102e-13], [0.028176438487433484, 0.9737897315715454, 2.3789063131700735e-15], [0.0031013597177395715, 1.0425169856310426, 0.001258925715929511], [0.0015461140408195586, 1.2052345863085565, 0.00134868764746486], [0.00031051785879065904, 1.6901314778574512, 0.0011832712576748628], [0.0005666599801514915, 1.0342370769473448, 0.0002195612705050585], [0.0009569976216924068, 0.9847738342913699, 0.00013723654148004898], [0.0012181085040403486, 0.9680190302600027, 8.24182077841877e-05], [0.0011097634762145296, 1.2298617957556142, 0.00034394872578617547], [0.006865268297209494, 1.0648883461564944, 0.0015278300081780468]], 'fee_twenty_ten_ratio': 1.4783940355540994, 'fee_thirty_ten_ratio': 1.8809530441304585, 'normal_adversarial_block_reward_per_min': 0.08327665429732464, 'noise_std_mempool_reward': None}\n",
      "Testing agent rank: 1 SpawnPoolWorker-36\n",
      "Testing agent rank: 0 SpawnPoolWorker-35\n",
      "Training agent rank: 2 SpawnPoolWorker-38\n",
      "Training agent rank: 1 SpawnPoolWorker-39\n",
      "Training agent rank: 0 SpawnPoolWorker-37\n",
      "Training agent rank: 3 SpawnPoolWorker-40\n",
      "Training agent rank: 6 SpawnPoolWorker-43\n",
      "Training agent rank: 5 SpawnPoolWorker-42\n",
      "Training agent rank: 8 SpawnPoolWorker-45\n",
      "Training agent rank: 4 SpawnPoolWorker-41\n",
      "Training agent rank: 7 SpawnPoolWorker-44\n",
      "Training agent rank: 9 SpawnPoolWorker-46\n",
      "Training agent rank: 10 SpawnPoolWorker-47\n",
      "Training agent rank: 13 SpawnPoolWorker-50\n",
      "Training agent rank: 14 SpawnPoolWorker-51\n",
      "Training agent rank: 11 SpawnPoolWorker-48\n",
      "Training agent rank: 12 SpawnPoolWorker-49\n",
      "Training agent rank: 17 SpawnPoolWorker-54\n",
      "Training agent rank: 20 SpawnPoolWorker-57\n",
      "Training agent rank: 18 SpawnPoolWorker-55\n",
      "Training agent rank: 15 SpawnPoolWorker-52\n",
      "Training agent rank: 19 SpawnPoolWorker-56\n",
      "Training agent rank: 21 SpawnPoolWorker-58\n",
      "Training agent rank: 22 SpawnPoolWorker-59\n",
      "Training agent rank: 29 SpawnPoolWorker-66\n",
      "Training agent rank: 23 SpawnPoolWorker-60\n",
      "Training agent rank: 25 SpawnPoolWorker-62\n",
      "Training agent rank: 27 SpawnPoolWorker-64\n",
      "Training agent rank: 28 SpawnPoolWorker-65\n",
      "Training agent rank: 16 SpawnPoolWorker-53\n",
      "Training agent rank: 24 SpawnPoolWorker-61\n",
      "Training agent rank: 26 SpawnPoolWorker-63\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.5638]])\n",
      "rank, episode_length 1 1000\n",
      "state: tensor([2., 0.])\n",
      "q_a tensor([[ 0.1772,  0.1102, -0.1821,  0.0277, -0.0109, -0.0058, -0.0094]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.5889, 0.0000, 0.4111, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[26.2512]])\n",
      "mu tensor([[0.0026]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6773]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.09464273  0.29247879  0.2482712   0.14344517  0.01012078\n",
      "  0.015021    0.01828992  0.02923186  0.12810772]\n",
      "log prob tensor([[-0.5295]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.6341]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[2.1514]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.6049]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[2.4538]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0135]])\n",
      "rank, episode_length 1 2000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[ 0.2909,  0.5022, -0.4872,  0.2044, -0.1622, -0.0112, -0.0114]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.6852, 0.0000, 0.3148, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0048]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6228]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [8.78379023e+00 2.55933089e-02 4.05690959e-03 2.72132769e-03\n",
      " 1.44606049e-03 7.31208562e-04 1.00555725e-03 1.18948490e-03\n",
      " 1.32680375e-03 7.70786898e-03]\n",
      "log prob tensor([[-0.3780]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.6290]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.0239]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0391]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0435]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0043]])\n",
      "rank, episode_length 1 3000\n",
      "state: tensor([2., 2.])\n",
      "q_a tensor([[-0.1786,  0.9044, -0.5352,  0.5957, -0.3787, -0.0117, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.8084, 0.1916, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[6.7487]])\n",
      "mu tensor([[0.0055]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4885]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.89986611  0.12364861  0.09685024  0.07610456  0.022082\n",
      "  0.03200033  0.03865232  0.07157036  0.2846933 ]\n",
      "log prob tensor([[-0.2127]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6127]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.4379]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0398]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.4180]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9996]])\n",
      "rank, episode_length 1 4000\n",
      "state: tensor([3., 1.])\n",
      "q_a tensor([[-0.6136,  1.2727, -0.4546,  0.8203, -0.4305, -0.0121, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.4603, 0.0000, 0.5397, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0057]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6900]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.92545925  0.12400365  0.09124546  0.05747234  0.02221189\n",
      "  0.03276976  0.03982176  0.06656262  0.28314672]\n",
      "log prob tensor([[-0.7758]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6353]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.8339]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0601]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.8039]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9917]])\n",
      "rank, episode_length 1 5000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-0.9893,  1.6511, -0.4032,  1.0309, -0.7144, -0.0115, -0.0121]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.3575, 0.0000, 0.6425, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0060]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6520]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [7.54471946e+00 5.89527134e-02 8.09486365e-03 5.20402010e-03\n",
      " 2.30157020e-03 1.46076300e-03 2.15627712e-03 2.61990441e-03\n",
      " 3.16336486e-03 1.69189135e-02]\n",
      "log prob tensor([[-1.0286]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.2855]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.0245]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0152]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0321]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0039]])\n",
      "rank, episode_length 1 6000\n",
      "state: tensor([2., 1.])\n",
      "q_a tensor([[-1.2871,  1.9456, -0.3934,  1.2248, -0.9611, -0.0118, -0.0126]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2903, 0.0000, 0.7097, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0061]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6025]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.09517911  0.14755142  0.1079554   0.06713699  0.02643853\n",
      "  0.03891785  0.04725205  0.07777229  0.33461039]\n",
      "log prob tensor([[-0.3430]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.5203]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1916]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0279]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1777]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9981]])\n",
      "rank, episode_length 1 7000\n",
      "state: tensor([0., 2.])\n",
      "q_a tensor([[-1.5474,  2.1296, -0.3051,  1.2803, -0.9634, -0.0122, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.1563, 0.7628, 0.0809, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0069]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.7000]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          3.38200077  0.14555593  0.10788936  0.06889723  0.02606109\n",
      "  0.03843654  0.04670355  0.07930888  0.33387037]\n",
      "log prob tensor([[-0.2707]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.3683]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-1.2742]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1363]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-1.2060]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0025]])\n",
      "rank, episode_length 1 8000\n",
      "state: tensor([4., 1.])\n",
      "q_a tensor([[-1.8203,  2.2060, -0.1848,  1.4551, -0.8748, -0.0117, -0.0126]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1631, 0.0000, 0.8369, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[24.1675]])\n",
      "mu tensor([[0.0071]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4447]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.97554693  0.13801872  0.1015158   0.06449051  0.02472561\n",
      "  0.03630698  0.04404601  0.07294991  0.31254925]\n",
      "log prob tensor([[-0.1780]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.3974]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.1234]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1326]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.1897]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9964]])\n",
      "rank, episode_length 1 9000\n",
      "state: tensor([2., 0.])\n",
      "q_a tensor([[-1.8759,  2.3380, -0.2499,  1.5301, -0.7267, -0.0122, -0.0126]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1644, 0.0000, 0.8356, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0066]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4469]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.02638145  0.05979706  0.04519728  0.03144052  0.01069767\n",
      "  0.01565768  0.01897657  0.03310935  0.13690426]\n",
      "log prob tensor([[-0.1796]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-2.9366]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.2069]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0689]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.2413]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 01m 24s, num steps 298944, FPS 3524, episode length 100000\n",
      "Episode reward: 1.0962807748791819\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.0962807748791819\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0036]])\n",
      "rank, episode_length 1 10000\n",
      "state: tensor([0., 1.])\n",
      "q_a tensor([[-1.8852,  2.4180, -0.3369,  1.6424, -0.7371, -0.0122, -0.0128]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.1214, 0.8786, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.5979]])\n",
      "mu tensor([[0.0064]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3697]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [1.00000000e+01 2.81581226e-01 9.78328657e-03 6.32477620e-03\n",
      " 2.78272779e-03 1.76461902e-03 2.62436973e-03 3.19712386e-03\n",
      " 4.00200196e-03 2.08115159e-02]\n",
      "log prob tensor([[-0.1294]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6905]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.3357]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0680]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.3018]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 01m 29s, num steps 319101, FPS 3546, episode length 100000\n",
      "Episode reward: 1.0737523465188383\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.0737523465188383\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0011]])\n",
      "rank, episode_length 1 11000\n",
      "state: tensor([4., 3.])\n",
      "q_a tensor([[-1.9362,  2.5415, -0.3413,  1.5282, -0.4981, -0.0121, -0.0122]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0106, 0.9369, 0.0524, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[33.1254]])\n",
      "mu tensor([[0.0077]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2640]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          3.02079674  0.30999621  0.23609207  0.16681741  0.05543197\n",
      "  0.08120518  0.09845163  0.17488876  0.71445365]\n",
      "log prob tensor([[-0.0652]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-1.3734]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0703]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0365]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0520]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9991]])\n",
      "rank, episode_length 1 12000\n",
      "state: tensor([4., 1.])\n",
      "q_a tensor([[-1.7942,  2.5018, -0.4463,  1.5198, -0.4197, -0.0123, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2062, 0.0000, 0.7938, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[12.2578]])\n",
      "mu tensor([[0.0071]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5089]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          5.49579896  0.18285095  0.14117445  0.10734859  0.03268007\n",
      "  0.04757484  0.05755649  0.10398202  0.4199735 ]\n",
      "log prob tensor([[-0.2310]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.4235]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.3401]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.4544]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.5673]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0022]])\n",
      "rank, episode_length 1 13000\n",
      "state: tensor([3., 2.])\n",
      "q_a tensor([[-1.7049,  2.4893, -0.4489,  1.3944, -0.3673, -0.0125, -0.0133]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0141, 0.9363, 0.0496, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0072]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2708]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.68731868  0.22875103  0.17247831  0.12011799  0.04092949\n",
      "  0.05991488  0.07261739  0.12604608  0.52294855]\n",
      "log prob tensor([[-0.0658]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.5527]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0450]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0136]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0382]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0014]])\n",
      "rank, episode_length 1 14000\n",
      "state: tensor([4., 2.])\n",
      "q_a tensor([[-1.6523,  2.3194, -0.3934,  1.3588, -0.2514, -0.0123, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0174, 0.9215, 0.0611, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[38.5651]])\n",
      "mu tensor([[0.0074]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3166]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.94578733  0.22375172  0.16956219  0.11855207  0.04002102\n",
      "  0.05869669  0.0711909   0.12533174  0.51493904]\n",
      "log prob tensor([[-0.0818]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-2.1263]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.6052]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1294]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.6699]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9974]])\n",
      "rank, episode_length 1 15000\n",
      "state: tensor([2., 0.])\n",
      "q_a tensor([[-1.8463,  2.3374, -0.2701,  1.3789, -0.2127, -0.0127, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1713, 0.0000, 0.8287, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[10.0590]])\n",
      "mu tensor([[0.0070]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4580]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          7.86139573  0.0790071   0.06051652  0.04368785  0.01412385\n",
      "  0.02066329  0.02504096  0.04490848  0.18228876]\n",
      "log prob tensor([[-0.1879]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5143]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.2689]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.3168]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.4273]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9970]])\n",
      "rank, episode_length 1 16000\n",
      "state: tensor([0., 1.])\n",
      "q_a tensor([[-1.9882,  2.4250, -0.2479,  1.4993, -0.3694, -0.0127, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.1484, 0.8516, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0064]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4199]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.57866243  0.07764609  0.05808895  0.038186    0.01389562\n",
      "  0.02045388  0.02483692  0.04287454  0.17854895]\n",
      "log prob tensor([[-0.1606]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6707]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1771]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1077]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1232]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9934]])\n",
      "rank, episode_length 1 17000\n",
      "state: tensor([4., 1.])\n",
      "q_a tensor([[-1.8582,  2.3076, -0.3473,  1.5675, -0.3080, -0.0125, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0143, 0.9210, 0.0647, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[1.9484]])\n",
      "mu tensor([[0.0066]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3138]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.5004025   0.19863879  0.15011984  0.10690719  0.03553974\n",
      "  0.05200751  0.06302893  0.10974487  0.45410375]\n",
      "log prob tensor([[-0.0823]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6865]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.4592]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0341]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.4422]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9950]])\n",
      "rank, episode_length 1 18000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-1.9837,  2.4634, -0.4137,  1.6752, -0.1792, -0.0129, -0.0130]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1722, 0.0000, 0.8278, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[8.1823]])\n",
      "mu tensor([[0.0067]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4594]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [9.06765961e+00 1.40625108e-02 2.73268687e-03 2.00284564e-03\n",
      " 1.27621825e-03 4.90433099e-04 6.11132648e-04 6.92869980e-04\n",
      " 8.05307119e-04 4.73852124e-03]\n",
      "log prob tensor([[-0.1890]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5762]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-1.5750]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1942]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-1.4779]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0022]])\n",
      "rank, episode_length 1 19000\n",
      "state: tensor([2., 1.])\n",
      "q_a tensor([[-1.9141,  2.3814, -0.4882,  1.7573, -0.0685, -0.0126, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0127, 0.9343, 0.0530, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[21.7919]])\n",
      "mu tensor([[0.0065]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2748]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.03683706  0.06915909  0.05067784  0.0313676   0.01239092\n",
      "  0.01820392  0.02208642  0.03638209  0.15657147]\n",
      "log prob tensor([[-0.0680]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.1931]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.3009]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0347]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.2836]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0000]])\n",
      "rank, episode_length 1 20000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-1.7640,  2.2677, -0.5680,  1.7698,  0.0299, -0.0129, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2322, 0.0000, 0.7678, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[10.4046]])\n",
      "mu tensor([[0.0064]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5419]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [8.92248217e+00 3.12188589e-02 4.72011964e-03 3.10400769e-03\n",
      " 1.55427259e-03 8.51416664e-04 1.19879541e-03 1.43123515e-03\n",
      " 1.60714665e-03 9.20770828e-03]\n",
      "log prob tensor([[-0.2642]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.4994]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.5992]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0708]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.6346]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 02m 50s, num steps 631284, FPS 3710, episode length 200000\n",
      "Episode reward: 1.3530081606610538\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.3530081606610538\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9991]])\n",
      "rank, episode_length 1 21000\n",
      "state: tensor([10.,  2.])\n",
      "q_a tensor([[-1.4613,  2.0293, -0.6334,  1.6449,  0.1497, -0.0094, -0.0137]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.3041, 0.0000, 0.6959, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[13.8040]])\n",
      "mu tensor([[0.0022]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6143]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.35738395  0.23817733  0.17268424  0.10756337  0.04270478\n",
      "  0.06282303  0.0762629   0.12255576  0.53546289]\n",
      "log prob tensor([[-1.1904]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.3276]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.5005]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0967]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.5489]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9974]])\n",
      "rank, episode_length 1 22000\n",
      "state: tensor([2., 1.])\n",
      "q_a tensor([[-2.0754,  2.4524, -0.3857,  1.7333,  0.0753, -0.0127, -0.0133]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1558, 0.0000, 0.8442, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0072]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4326]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.11576378  0.15483883  0.12660638  0.1170739   0.02759665\n",
      "  0.03965788  0.04777604  0.0951395   0.36015568]\n",
      "log prob tensor([[-0.1694]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.1226]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0800]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0349]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0626]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 03m 06s, num steps 693475, FPS 3725, episode length 200000\n",
      "Episode reward: 1.351749630826341\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.351749630826341\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9979]])\n",
      "rank, episode_length 1 23000\n",
      "state: tensor([3., 3.])\n",
      "q_a tensor([[-1.9236,  2.5481, -0.6329,  1.8900, -0.0981, -0.0130, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.9601, 0.0399, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[18.4512]])\n",
      "mu tensor([[0.0076]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.1676]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.17905921  0.22714459  0.17659759  0.13488691  0.04057782\n",
      "  0.05909138  0.07149873  0.1312644   0.52469145]\n",
      "log prob tensor([[-0.0407]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.0684]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0922]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0491]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0676]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9978]])\n",
      "rank, episode_length 1 24000\n",
      "state: tensor([0., 1.])\n",
      "q_a tensor([[-1.9508,  2.5306, -0.6062,  1.9311, -0.2695, -0.0130, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.0733, 0.9267, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0067]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2620]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [9.12136235e+00 6.14560470e-02 8.40608752e-03 5.40765213e-03\n",
      " 2.38527187e-03 1.51681315e-03 2.24300003e-03 2.72700292e-03\n",
      " 3.31540510e-03 1.76350140e-02]\n",
      "log prob tensor([[-0.0761]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.6452]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-1.1221]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.2557]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.9943]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0011]])\n",
      "rank, episode_length 1 25000\n",
      "state: tensor([0., 1.])\n",
      "q_a tensor([[-1.9652,  2.4843, -0.5897,  1.9915, -0.3707, -0.0130, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.0704, 0.9296, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0068]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2546]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [9.44705414e+00 1.18340716e-01 1.56730174e-02 1.04818193e-02\n",
      " 4.93136180e-03 2.82126804e-03 4.22220810e-03 5.15513585e-03\n",
      " 7.14178354e-03 3.45048073e-02]\n",
      "log prob tensor([[-0.0730]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6766]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.3326]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0587]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.3033]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9997]])\n",
      "rank, episode_length 1 26000\n",
      "state: tensor([5., 1.])\n",
      "q_a tensor([[-1.8425,  2.2499, -0.5453,  1.9823, -0.4086, -0.0121, -0.0128]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2146, 0.0000, 0.7854, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[3.8871]])\n",
      "mu tensor([[0.0072]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5200]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.79098113  0.22868363  0.17231371  0.12007213  0.04091804\n",
      "  0.06003592  0.07282513  0.126628    0.52437908]\n",
      "log prob tensor([[-0.2416]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6691]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.1022]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1170]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.1607]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0024]])\n",
      "rank, episode_length 1 27000\n",
      "state: tensor([6., 1.])\n",
      "q_a tensor([[-1.5561,  2.0630, -0.5882,  1.7871, -0.5000, -0.0118, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2753, 0.0000, 0.7247, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0050]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5885]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          6.52328672  0.63649233  0.41497049  0.14690958  0.04822182\n",
      "  0.07054982  0.08549581  0.1499401   0.61740045]\n",
      "log prob tensor([[-0.3220]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.3853]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0990]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0264]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0858]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9994]])\n",
      "rank, episode_length 1 28000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-1.7429,  2.2010, -0.5356,  1.8541, -0.7412, -0.0128, -0.0138]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2302, 0.0000, 0.7698, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[16.8835]])\n",
      "mu tensor([[0.0071]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5395]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          5.1289965   0.40916368  0.35533889  0.38755599  0.07269103\n",
      "  0.1030361   0.12324381  0.05118362  0.18981783]\n",
      "log prob tensor([[-0.2616]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.1704]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.2050]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1035]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1533]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9971]])\n",
      "rank, episode_length 1 29000\n",
      "state: tensor([2., 1.])\n",
      "q_a tensor([[-1.7984,  2.1987, -0.4668,  1.8087, -0.7961, -0.0122, -0.0131]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0169, 0.9192, 0.0639, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0072]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3222]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.10514161  0.15245781  0.12410149  0.10982671  0.02717304\n",
      "  0.03916596  0.04722975  0.09380273  0.35607021]\n",
      "log prob tensor([[-0.0843]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.0263]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1107]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0480]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0867]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0010]])\n",
      "rank, episode_length 1 30000\n",
      "state: tensor([2., 0.])\n",
      "q_a tensor([[-1.9507,  2.2242, -0.3563,  1.8321, -0.9598, -0.0133, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1688, 0.0000, 0.8312, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[1.0752]])\n",
      "mu tensor([[0.0079]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4539]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [1.00000000e+01 8.33437454e-01 2.77356618e-02 1.84198496e-02\n",
      " 8.55961593e-03 4.99476885e-03 7.46448494e-03 9.10930284e-03\n",
      " 1.23628753e-02 6.06174167e-02]\n",
      "log prob tensor([[-0.1848]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6901]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.4643]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0831]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.4228]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0006]])\n",
      "rank, episode_length 1 31000\n",
      "state: tensor([3., 3.])\n",
      "q_a tensor([[-1.9373,  2.2687, -0.3720,  1.7961, -1.1306, -0.0127, -0.0130]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.9334, 0.0666, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[8.4021]])\n",
      "mu tensor([[0.0076]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2446]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.59097074  0.13386712  0.09551537  0.0551845   0.02401955\n",
      "  0.03531588  0.04285515  0.06660254  0.2983029 ]\n",
      "log prob tensor([[-0.0689]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5719]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0771]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0501]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0520]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9983]])\n",
      "rank, episode_length 1 32000\n",
      "state: tensor([6., 2.])\n",
      "q_a tensor([[-1.6962,  2.2237, -0.4944,  1.6492, -1.1678, -0.0108, -0.0120]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2311, 0.0000, 0.7689, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[36.4425]])\n",
      "mu tensor([[0.0067]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5407]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          5.91869694  0.63498945  0.57853489  0.33689147  0.04205879\n",
      "  0.06082659  0.07341875  0.13689887  0.54205973]\n",
      "log prob tensor([[-0.2628]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-1.8279]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.8339]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.3493]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[1.0085]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 04m 20s, num steps 973999, FPS 3745, episode length 300000\n",
      "Episode reward: 1.3527975954829095\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.3530081606610538\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9992]])\n",
      "rank, episode_length 1 33000\n",
      "state: tensor([3., 1.])\n",
      "q_a tensor([[-1.9628,  2.5418, -0.4737,  1.6949, -1.2693, -0.0126, -0.0130]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0104, 0.9433, 0.0462, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0075]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2448]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.92332812  0.12370265  0.09088594  0.05830113  0.02216298\n",
      "  0.03261576  0.03960081  0.06557912  0.28067252]\n",
      "log prob tensor([[-0.0583]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.6948]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0836]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0357]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0658]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9975]])\n",
      "rank, episode_length 1 34000\n",
      "state: tensor([4., 0.])\n",
      "q_a tensor([[-2.0794,  2.5303, -0.3292,  1.5663, -1.1691, -0.0123, -0.0124]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1480, 0.0000, 0.8520, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[6.7002]])\n",
      "mu tensor([[0.0090]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4193]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.56943038  0.18897671  0.14272848  0.10095252  0.03381309\n",
      "  0.04932134  0.05970226  0.10336097  0.42978429]\n",
      "log prob tensor([[-0.1602]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6206]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.1555]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0436]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.1773]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 04m 37s, num steps 1038547, FPS 3743, episode length 300000\n",
      "Episode reward: 1.3606705984273288\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.3606705984273288\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9993]])\n",
      "rank, episode_length 1 35000\n",
      "state: tensor([0., 1.])\n",
      "q_a tensor([[-2.1024,  2.6196, -0.4020,  1.6748, -1.2992, -0.0127, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.1114, 0.8886, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0072]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3494]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [9.88740808e+00 1.18856497e-01 1.57402845e-02 1.05311118e-02\n",
      " 4.95976003e-03 2.83331292e-03 4.24021345e-03 5.17711366e-03\n",
      " 7.17922397e-03 3.46620132e-02]\n",
      "log prob tensor([[-0.1181]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.3931]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.5537]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.2065]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.4505]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0004]])\n",
      "rank, episode_length 1 36000\n",
      "state: tensor([3., 2.])\n",
      "q_a tensor([[-1.9063,  2.5273, -0.5359,  1.7493, -1.4042, -0.0125, -0.0133]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2025, 0.0000, 0.7975, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0077]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5039]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.53764526  0.07241069  0.04987458  0.02613558  0.01301677\n",
      "  0.01927131  0.02344172  0.03398844  0.1594283 ]\n",
      "log prob tensor([[-0.2263]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.2277]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.0438]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0486]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0681]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0015]])\n",
      "rank, episode_length 1 37000\n",
      "state: tensor([4., 4.])\n",
      "q_a tensor([[-1.8317,  2.2094, -0.3881,  1.6435, -1.3463, -0.0121, -0.0121]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.9307, 0.0693, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[2.4918]])\n",
      "mu tensor([[0.0085]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2518]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          4.28064428  0.50519909  0.32089563  0.27328967  0.07167904\n",
      "  0.10381892  0.12540163  0.24099611  0.93393086]\n",
      "log prob tensor([[-0.0718]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6841]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0024]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0208]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0080]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0001]])\n",
      "rank, episode_length 1 38000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-2.0199,  2.3340, -0.2875,  1.6094, -1.3027, -0.0130, -0.0127]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1503, 0.0000, 0.8497, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0080]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4232]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [9.85735105e+00 8.46317866e-02 1.13261680e-02 7.38002305e-03\n",
      " 3.27787672e-03 2.04186010e-03 3.04759185e-03 3.71740500e-03\n",
      " 4.79520009e-03 2.43832459e-02]\n",
      "log prob tensor([[-0.1629]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.3923]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.0233]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0485]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0475]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9965]])\n",
      "rank, episode_length 1 39000\n",
      "state: tensor([3., 0.])\n",
      "q_a tensor([[-1.9854,  2.1160, -0.1738,  1.5245, -1.0664, -0.0125, -0.0123]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1405, 0.0000, 0.8595, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0096]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4058]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          3.19823108  0.26132499  0.22724093  0.24760238  0.04642132\n",
      "  0.06586308  0.07901099  0.17534884  0.61941348]\n",
      "log prob tensor([[-1.9629]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5330]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1364]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0534]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9976]])\n",
      "rank, episode_length 1 40000\n",
      "state: tensor([11.,  1.])\n",
      "q_a tensor([[-1.4469,  1.6664, -0.3121,  1.3316, -0.7973, -0.0112, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2433, 0.0000, 0.7567, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0017]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5548]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          5.28661258  0.53221428  0.43108823  0.38249303  0.09489284\n",
      "  0.1366303   0.1646948   0.32310742  1.23613416]\n",
      "log prob tensor([[-0.2788]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5414]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.3596]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1565]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.4379]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9975]])\n",
      "rank, episode_length 1 41000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-2.1429,  2.5531, -0.3863,  1.6515, -0.8449, -0.0132, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1472, 0.0000, 0.8528, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[15.3833]])\n",
      "mu tensor([[0.0073]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4178]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.54506284  0.35469192  0.22255357  0.1522696   0.02520317\n",
      "  0.03534204  0.0422267   0.10047912  0.34015561]\n",
      "log prob tensor([[-0.1592]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.2625]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.4547]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0723]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.4908]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9994]])\n",
      "rank, episode_length 1 42000\n",
      "state: tensor([0., 1.])\n",
      "q_a tensor([[-1.9499,  2.6075, -0.6001,  1.6951, -0.8143, -0.0125, -0.0138]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.0915, 0.9085, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[18.6747]])\n",
      "mu tensor([[0.0062]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3060]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.10748003  0.09232286  0.07285665  0.05798771  0.01647985\n",
      "  0.02394239  0.02894782  0.05462041  0.21442965]\n",
      "log prob tensor([[-0.0960]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.0450]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1391]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0105]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1339]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0010]])\n",
      "rank, episode_length 1 43000\n",
      "state: tensor([4., 0.])\n",
      "q_a tensor([[-1.6853,  2.1845, -0.5242,  1.5788, -0.7301, -0.0151, -0.0136]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2385, 0.0000, 0.7615, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0070]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5493]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.35649189  0.18184016  0.13242467  0.08261782  0.03259246\n",
      "  0.0481272   0.0585023   0.0956141   0.41266812]\n",
      "log prob tensor([[-1.4336]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.0660]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1166]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0085]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1123]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 05m 51s, num steps 1307552, FPS 3721, episode length 400000\n",
      "Episode reward: 1.3509858752119486\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.3530081606610538\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9995]])\n",
      "rank, episode_length 1 44000\n",
      "state: tensor([7., 0.])\n",
      "q_a tensor([[-1.3012,  1.8084, -0.5805,  1.4630, -0.6302, -0.0118, -0.0137]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.3272, 0.0000, 0.6728, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[8.8874]])\n",
      "mu tensor([[0.0023]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6322]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          3.7626975   0.45800737  0.36298958  0.30549379  0.08175866\n",
      "  0.11818558  0.14264763  0.26888087  1.0555865 ]\n",
      "log prob tensor([[-0.3964]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5426]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[1.0686]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1131]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[1.1252]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 06m 03s, num steps 1348287, FPS 3713, episode length 400000\n",
      "Episode reward: 1.3513607130736898\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.3606705984273288\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0015]])\n",
      "rank, episode_length 1 45000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-1.9618,  2.3628, -0.5164,  1.7652, -0.6547, -0.0140, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1907, 0.0000, 0.8093, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0072]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4873]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [8.78156390e+00 7.05126990e-02 9.53930013e-03 6.16048992e-03\n",
      " 2.70917126e-03 1.72074218e-03 2.55707146e-03 3.11425905e-03\n",
      " 3.87880504e-03 2.02479039e-02]\n",
      "log prob tensor([[-0.2116]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6770]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.7662]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.2071]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.6627]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9951]])\n",
      "rank, episode_length 1 46000\n",
      "state: tensor([0., 2.])\n",
      "q_a tensor([[-2.0796,  2.3973, -0.4318,  1.7150, -0.6632, -0.0128, -0.0140]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.0966, 0.8267, 0.0766, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[5.2805]])\n",
      "mu tensor([[0.0068]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5800]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.00326616  0.13821941  0.11305448  0.09873027  0.02462512\n",
      "  0.03549005  0.04279413  0.08600377  0.32431668]\n",
      "log prob tensor([[-2.3370]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6467]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-2.7344]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.4714]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-2.4987]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9995]])\n",
      "rank, episode_length 1 47000\n",
      "state: tensor([3., 0.])\n",
      "q_a tensor([[-2.2610,  2.2171, -0.1762,  1.6591, -0.5645, -0.0136, -0.0128]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1106, 0.0000, 0.8894, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[18.8529]])\n",
      "mu tensor([[0.0094]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3477]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.57535759  0.12416868  0.09830508  0.08026044  0.02216276\n",
      "  0.03214835  0.03884953  0.07353848  0.28796551]\n",
      "log prob tensor([[-0.1172]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.0505]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.0706]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1219]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.1315]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0029]])\n",
      "rank, episode_length 1 48000\n",
      "state: tensor([4., 1.])\n",
      "q_a tensor([[-2.0380,  2.0653, -0.2205,  1.5582, -0.4551, -0.0121, -0.0126]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1397, 0.0000, 0.8603, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0088]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4045]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.60841538  0.08126389  0.05768592  0.03269398  0.01458457\n",
      "  0.02145526  0.02603951  0.04006586  0.18072677]\n",
      "log prob tensor([[-0.1505]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.3198]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0556]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0153]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0480]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9956]])\n",
      "rank, episode_length 1 49000\n",
      "state: tensor([3., 1.])\n",
      "q_a tensor([[-1.9847,  2.0068, -0.2413,  1.6046, -0.5358, -0.0131, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1489, 0.0000, 0.8511, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[33.4653]])\n",
      "mu tensor([[0.0090]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4208]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.24711349  0.30632455  0.23698175  0.17941007  0.05473797\n",
      "  0.07972669  0.09646961  0.17534026  0.70559185]\n",
      "log prob tensor([[-0.1612]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-1.4042]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1727]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0272]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1591]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9977]])\n",
      "rank, episode_length 1 50000\n",
      "state: tensor([0., 1.])\n",
      "q_a tensor([[-2.0406,  2.3276, -0.4626,  1.7891, -0.6036, -0.0129, -0.0139]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.0952, 0.9048, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[9.7964]])\n",
      "mu tensor([[0.0064]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3144]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [1.00000000e+01 2.18532837e+00 5.17153303e-02 3.67964907e-02\n",
      " 2.08223916e-02 9.27907775e-03 1.37854805e-02 1.67910630e-02\n",
      " 2.64186919e-02 1.16997122e-01]\n",
      "log prob tensor([[-2.3517]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5221]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-1.5489]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.4775]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-1.3102]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9988]])\n",
      "rank, episode_length 1 51000\n",
      "state: tensor([6., 0.])\n",
      "q_a tensor([[-1.3939,  1.3358, -0.3769,  1.6348, -0.5039, -0.0135, -0.0127]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2656, 0.0000, 0.7344, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0053]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5788]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.05978086  0.28607625  0.23254088  0.20889821  0.05099727\n",
      "  0.07327661  0.08826037  0.17406403  0.66402005]\n",
      "log prob tensor([[-0.3087]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6656]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.7128]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1000]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.7628]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0026]])\n",
      "rank, episode_length 1 52000\n",
      "state: tensor([1., 1.])\n",
      "q_a tensor([[-1.9722,  2.3279, -0.6281,  1.9952, -0.5798, -0.0124, -0.0147]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0., 0., 1., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0054]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[-0.]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.00304489  0.1214699   0.09458869  0.07285583  0.02169819\n",
      "  0.0316128   0.03825797  0.07048248  0.28098268]\n",
      "log prob tensor([[0.]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-1.0838]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.5895]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0438]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.5676]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 07m 13s, num steps 1591754, FPS 3673, episode length 500000\n",
      "Episode reward: 1.3614261357361666\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.3614261357361666\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0050]])\n",
      "rank, episode_length 1 53000\n",
      "state: tensor([2., 1.])\n",
      "q_a tensor([[-2.1670,  2.3476, -0.4861,  1.9868, -0.7150, -0.0134, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1570, 0.0000, 0.8430, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[4.9390]])\n",
      "mu tensor([[0.0070]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4346]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.63224397  0.08405624  0.06020447  0.03462345  0.01507639\n",
      "  0.02238138  0.02725427  0.04346403  0.19075786]\n",
      "log prob tensor([[-0.1708]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6530]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1361]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0628]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1047]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0004]])\n",
      "rank, episode_length 1 54000\n",
      "state: tensor([2., 1.])\n",
      "q_a tensor([[-2.1539,  2.3758, -0.4422,  1.8168, -0.7746, -0.0133, -0.0132]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0101, 0.9341, 0.0558, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[8.5453]])\n",
      "mu tensor([[0.0074]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2710]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.16112109  0.12384815  0.09596054  0.07134685  0.02212668\n",
      "  0.03227183  0.03906778  0.07145893  0.2864364 ]\n",
      "log prob tensor([[-0.0681]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5670]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.0989]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1311]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.1645]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 07m 28s, num steps 1645892, FPS 3666, episode length 500000\n",
      "Episode reward: 1.3526314844068053\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.3606705984273288\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9990]])\n",
      "rank, episode_length 1 55000\n",
      "state: tensor([1., 1.])\n",
      "q_a tensor([[-2.0817,  2.6248, -0.6347,  1.8159, -0.8604, -0.0128, -0.0146]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.9630, 0.0370, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0057]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.1582]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.76679096  0.08113544  0.06515968  0.05460552  0.01447058\n",
      "  0.02088529  0.02519428  0.0488341   0.18849825]\n",
      "log prob tensor([[-0.0377]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.0882]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.2248]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1443]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1526]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9989]])\n",
      "rank, episode_length 1 56000\n",
      "state: tensor([3., 0.])\n",
      "q_a tensor([[-1.9492,  2.2858, -0.5178,  1.7168, -0.8340, -0.0136, -0.0131]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1929, 0.0000, 0.8071, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0094]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4904]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          5.32481947  0.1331475   0.10295786  0.07597725  0.02378993\n",
      "  0.03474221  0.04207707  0.07679903  0.30823847]\n",
      "log prob tensor([[-0.2143]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.4916]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.7841]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.2759]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.9220]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9961]])\n",
      "rank, episode_length 1 57000\n",
      "state: tensor([2., 0.])\n",
      "q_a tensor([[-2.0482,  2.2285, -0.4077,  1.7020, -0.6829, -0.0141, -0.0128]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1624, 0.0000, 0.8376, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0090]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4436]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.7237325   0.09790557  0.07554957  0.05453078  0.0174935\n",
      "  0.02557378  0.03098308  0.05646691  0.22696308]\n",
      "log prob tensor([[-1.8177]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.1224]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0037]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0830]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0378]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9980]])\n",
      "rank, episode_length 1 58000\n",
      "state: tensor([0., 1.])\n",
      "q_a tensor([[-1.9831,  2.3897, -0.5152,  1.5826, -0.5708, -0.0128, -0.0141]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.1093, 0.8907, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[29.3496]])\n",
      "mu tensor([[0.0064]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3451]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [1.00000000e+01 6.00532207e-01 5.08525787e-02 3.94568422e-02\n",
      " 2.89682737e-02 9.08366964e-03 1.32623900e-02 1.60607508e-02\n",
      " 2.95481774e-02 1.18038417e-01]\n",
      "log prob tensor([[-0.1158]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.9339]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0542]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0392]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0346]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0002]])\n",
      "rank, episode_length 1 59000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-2.1018,  2.2937, -0.3877,  1.6364, -0.5679, -0.0146, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1526, 0.0000, 0.8474, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[5.5255]])\n",
      "mu tensor([[0.0078]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4273]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [1.00000000e+01 3.45802968e-01 4.66895988e-02 3.57835132e-02\n",
      " 2.52867660e-02 8.34525679e-03 1.22191710e-02 1.48114939e-02\n",
      " 2.66785183e-02 1.08057693e-01]\n",
      "log prob tensor([[-0.1656]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6436]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.1139]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0935]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.1607]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0000]])\n",
      "rank, episode_length 1 60000\n",
      "state: tensor([2., 1.])\n",
      "q_a tensor([[-1.9939,  2.4102, -0.5914,  1.7170, -0.4601, -0.0127, -0.0144]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1974, 0.0000, 0.8026, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[4.9036]])\n",
      "mu tensor([[0.0059]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4968]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [1.00000000e+01 3.97876409e-01 5.27957446e-02 3.58235703e-02\n",
      " 1.78437448e-02 9.49795230e-03 1.40800888e-02 1.71340946e-02\n",
      " 2.40554476e-02 1.15417518e-01]\n",
      "log prob tensor([[-0.2199]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6522]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.5020]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0514]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.5276]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0002]])\n",
      "rank, episode_length 1 61000\n",
      "state: tensor([7., 2.])\n",
      "q_a tensor([[-1.2684,  1.2296, -0.3590,  1.3987, -0.2641, -0.0124, -0.0143]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2871, 0.0000, 0.7129, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0025]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5996]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.46270489  0.19519332  0.13522186  0.07187531  0.03507566\n",
      "  0.05213949  0.06351614  0.09407201  0.43419907]\n",
      "log prob tensor([[-1.2478]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.1094]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.3310]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0207]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.3413]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9968]])\n",
      "rank, episode_length 1 62000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-2.3725,  2.4052, -0.2695,  1.7096, -0.2773, -0.0142, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1088, 0.0000, 0.8912, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0081]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3440]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          3.3086821   0.20251245  0.19378295  0.27034687  0.03579261\n",
      "  0.04942376  0.05874736  0.15277744  0.4887608 ]\n",
      "log prob tensor([[-0.1152]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6713]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0002]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0374]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0185]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 08m 35s, num steps 1879353, FPS 3644, episode length 600000\n",
      "Episode reward: 1.3461522184537158\n",
      "Max profit so far: 1.3614261357361666\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9992]])\n",
      "rank, episode_length 1 63000\n",
      "state: tensor([7., 1.])\n",
      "q_a tensor([[-1.2343,  0.9196, -0.1741,  1.3887, -0.2820, -0.0131, -0.0133]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2573, 0.0000, 0.7427, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[34.7276]])\n",
      "mu tensor([[0.0033]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5702]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.97200978  0.41166669  0.33535318  0.30273009  0.07337635\n",
      "  0.1055544   0.12719764  0.25220865  0.95830063]\n",
      "log prob tensor([[-0.2974]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-1.6297]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.1033]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1225]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.1645]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0024]])\n",
      "rank, episode_length 1 64000\n",
      "state: tensor([1., 1.])\n",
      "q_a tensor([[-1.7278,  2.1053, -0.5836,  1.6905, -0.3600, -0.0136, -0.0160]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.9364, 0.0636, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0027]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.2369]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.70828525  0.10664778  0.07519024  0.04217168  0.01914761\n",
      "  0.02831358  0.03442882  0.05254687  0.23783481]\n",
      "log prob tensor([[-0.0657]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5561]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-1.4509]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.2916]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-1.3051]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 08m 55s, num steps 1950121, FPS 3640, episode length 600000\n",
      "Episode reward: 1.3626750168347745\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.3626750168347745\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9999]])\n",
      "rank, episode_length 1 65000\n",
      "state: tensor([4., 2.])\n",
      "q_a tensor([[-1.3944,  1.1603, -0.1897,  1.4484, -0.3693, -0.0135, -0.0131]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0581, 0.7480, 0.1939, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[5.3503]])\n",
      "mu tensor([[0.0058]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.7007]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.00864157  0.27491304  0.21665955  0.17326787  0.0490788\n",
      "  0.0712245   0.08608047  0.16166065  0.63662574]\n",
      "log prob tensor([[-0.2904]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6439]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.2889]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0623]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.3201]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9991]])\n",
      "rank, episode_length 1 66000\n",
      "state: tensor([2., 0.])\n",
      "q_a tensor([[-1.6023,  1.5473, -0.2253,  1.4464, -0.4767, -0.0149, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.2015, 0.0000, 0.7985, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0068]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.5025]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.43446713  0.19372419  0.15906623  0.14188226  0.03450909\n",
      "  0.04967332  0.05987271  0.12096485  0.45441888]\n",
      "log prob tensor([[-1.6020]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5302]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.2509]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.2466]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.3742]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9983]])\n",
      "rank, episode_length 1 67000\n",
      "state: tensor([4., 2.])\n",
      "q_a tensor([[-1.3288,  1.1621, -0.1782,  1.2989, -0.4415, -0.0142, -0.0135]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0616, 0.7437, 0.1947, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[3.9934]])\n",
      "mu tensor([[0.0060]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.7105]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.27944887  0.17406503  0.13251296  0.09485479  0.03112994\n",
      "  0.04545177  0.0550399   0.09709526  0.39860025]\n",
      "log prob tensor([[-0.2961]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6664]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.0046]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0296]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0194]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0024]])\n",
      "rank, episode_length 1 68000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-2.0199,  2.3400, -0.3538,  1.3808, -0.3359, -0.0143, -0.0134]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1589, 0.0000, 0.8411, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[9.0726]])\n",
      "mu tensor([[0.0078]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4379]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.44014525  0.06007698  0.04776447  0.03781999  0.01071816\n",
      "  0.01555728  0.01880311  0.03605837  0.1402161 ]\n",
      "log prob tensor([[-0.1731]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5510]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1453]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0594]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1156]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9981]])\n",
      "rank, episode_length 1 69000\n",
      "state: tensor([7., 1.])\n",
      "q_a tensor([[-0.9924,  0.8432, -0.1820,  0.9909, -0.1817, -0.0135, -0.0130]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.3078, 0.0000, 0.6922, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[6.9795]])\n",
      "mu tensor([[0.0036]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6173]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          6.42145904  0.88173505  0.70001988  0.46365705  0.11460003\n",
      "  0.16702402  0.20215427  0.37269122  1.4855558 ]\n",
      "log prob tensor([[-0.3679]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6031]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.0989]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0127]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.1052]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0018]])\n",
      "rank, episode_length 1 70000\n",
      "state: tensor([1., 1.])\n",
      "q_a tensor([[-2.0071,  2.5352, -0.4677,  1.2626, -0.2153, -0.0131, -0.0146]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.9527, 0.0473, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[3.7096]])\n",
      "mu tensor([[0.0061]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.1905]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          0.48162298  0.06436312  0.04716573  0.02925396  0.011531\n",
      "  0.01704008  0.02071871  0.03443088  0.14701211]\n",
      "log prob tensor([[-0.0484]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6702]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1588]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0428]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1374]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0010]])\n",
      "rank, episode_length 1 71000\n",
      "state: tensor([2., 0.])\n",
      "q_a tensor([[-2.2045,  2.1151, -0.1504,  1.4448, -0.2818, -0.0143, -0.0127]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1136, 0.0000, 0.8864, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[16.0918]])\n",
      "mu tensor([[0.0095]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3540]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.89194196  0.10660131  0.08027545  0.07093444  0.01754788\n",
      "  0.02527046  0.03046371  0.06064499  0.22991923]\n",
      "log prob tensor([[-2.1748]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.2314]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.4156]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0847]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.4580]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 09m 58s, num steps 2171818, FPS 3627, episode length 700000\n",
      "Episode reward: 1.363601383781054\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.363601383781054\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0012]])\n",
      "rank, episode_length 1 72000\n",
      "state: tensor([5., 2.])\n",
      "q_a tensor([[-1.3389,  1.1413, -0.1848,  1.2088, -0.3086, -0.0143, -0.0133]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0621, 0.7412, 0.1968, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[12.8351]])\n",
      "mu tensor([[0.0052]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.7144]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.84324214  0.38271543  0.30057064  0.23796587  0.06833662\n",
      "  0.09911142  0.1197518   0.22321819  0.88379733]\n",
      "log prob tensor([[-0.2995]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.3892]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.5081]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.2018]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.6090]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0009]])\n",
      "rank, episode_length 1 73000\n",
      "state: tensor([0., 1.])\n",
      "q_a tensor([[-1.9278,  2.5742, -0.6919,  1.5645, -0.4295, -0.0126, -0.0152]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.0948, 0.9052, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0055]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.3135]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [9.56588069 0.72645481 0.1016359  0.08358759 0.07577065 0.01810618\n",
      " 0.02588953 0.03112926 0.06257414 0.23607732]\n",
      "log prob tensor([[-0.0996]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-1.0030]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.2079]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1619]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.1269]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0007]])\n",
      "rank, episode_length 1 74000\n",
      "state: tensor([0., 2.])\n",
      "q_a tensor([[-1.9520,  2.6464, -0.7228,  1.5756, -0.4723, -0.0120, -0.0160]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20, -1.0000e+20,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.0000, 0.0817, 0.8134, 0.1049, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0059]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6092]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [1.00000000e+01 4.87362047e+00 4.59914660e-02 3.31793667e-02\n",
      " 2.00428229e-02 8.24849807e-03 1.20636662e-02 1.46123971e-02\n",
      " 2.30254639e-02 1.02213952e-01]\n",
      "log prob tensor([[-0.2065]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.3996]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.5124]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0770]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.4739]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 10m 23s, num steps 2257645, FPS 3623, episode length 700000\n",
      "Episode reward: 1.3545454656076574\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.3626750168347745\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9967]])\n",
      "rank, episode_length 1 75000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-2.2223,  2.4088, -0.4549,  1.7883, -0.4387, -0.0143, -0.0129]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1459, 0.0000, 0.8541, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[12.1451]])\n",
      "mu tensor([[0.0085]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4155]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [9.18492732e+00 1.38909520e-01 1.83707814e-02 1.24854550e-02\n",
      " 6.13327601e-03 3.30400180e-03 4.94089253e-03 6.03117298e-03\n",
      " 8.66676241e-03 4.08212272e-02]\n",
      "log prob tensor([[-0.1577]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.4336]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.2663]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0335]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.2495]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9960]])\n",
      "rank, episode_length 1 76000\n",
      "state: tensor([5., 1.])\n",
      "q_a tensor([[-1.5648,  0.9157,  0.0074,  1.5279, -0.4221, -0.0151, -0.0108]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0563, 0.6725, 0.2712, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0063]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.7826]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.88530156  0.19981296  0.14558284  0.09127374  0.03581395\n",
      "  0.05284647  0.06422328  0.1049107   0.45296027]\n",
      "log prob tensor([[-1.3050]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.3857]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.1055]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0323]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.1217]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0008]])\n",
      "rank, episode_length 1 77000\n",
      "state: tensor([1., 1.])\n",
      "q_a tensor([[-1.9367,  2.7351, -0.8289,  1.6456, -0.3740, -0.0124, -0.0162]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.9725, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[23.8899]])\n",
      "mu tensor([[0.0052]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.1261]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          3.25650121  0.10397774  0.08175706  0.06335704  0.01856192\n",
      "  0.02701046  0.03267405  0.06141743  0.24183685]\n",
      "log prob tensor([[-0.0279]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.3855]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0116]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0317]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.0043]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0014]])\n",
      "rank, episode_length 1 78000\n",
      "state: tensor([4., 3.])\n",
      "q_a tensor([[-1.4643,  1.2119, -0.1853,  1.4117, -0.4302, -0.0138, -0.0122]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0523, 0.7598, 0.1879, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[3.9952]])\n",
      "mu tensor([[0.0059]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.6772]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.10072663  0.14710871  0.10108032  0.05181551  0.02644521\n",
      "  0.03930548  0.04787662  0.06965397  0.32572941]\n",
      "log prob tensor([[-0.2747]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6663]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-1.1103]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.2491]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.9858]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0032]])\n",
      "rank, episode_length 1 79000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-2.0273,  2.1568, -0.3812,  1.6563, -0.5933, -0.0140, -0.0136]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1616, 0.0000, 0.8384, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[17.0065]])\n",
      "mu tensor([[0.0084]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4424]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [1.00000000e+01 3.65093936e-01 4.94081166e-02 3.81766065e-02\n",
      " 2.76682047e-02 8.82751465e-03 1.29009871e-02 1.56281684e-02\n",
      " 2.85473672e-02 1.14573209e-01]\n",
      "log prob tensor([[-0.1763]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.1692]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.1161]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1057]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0632]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0022]])\n",
      "rank, episode_length 1 80000\n",
      "state: tensor([3., 1.])\n",
      "q_a tensor([[-1.5239,  0.8377,  0.0907,  1.4294, -0.5660, -0.0156, -0.0113]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0601, 0.6377, 0.3021, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0059]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.8175]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          1.38998001  0.19275121  0.15889482  0.15185455  0.03434084\n",
      "  0.04923886  0.05927401  0.119585    0.44877784]\n",
      "log prob tensor([[-0.4498]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6244]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.1368]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1509]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.2123]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0020]])\n",
      "rank, episode_length 1 81000\n",
      "state: tensor([4., 1.])\n",
      "q_a tensor([[-1.4547,  0.6758,  0.1723,  1.3409, -0.6135, -0.0155, -0.0107]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1642, 0.0000, 0.8358, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[12.1905]])\n",
      "mu tensor([[0.0068]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4466]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          3.16265117  0.4373116   0.3580812   0.34634039  0.0779442\n",
      "  0.11222966  0.13531043  0.27021883  1.01922473]\n",
      "log prob tensor([[-0.1794]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.4258]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[0.8786]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1763]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[0.9667]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "**************************  TEST RESULT  **************************\n",
      "Time 00h 11m 20s, num steps 2460369, FPS 3614, episode length 800000\n",
      "Episode reward: 1.3550796200882351\n",
      "... saving checkpoint ...\n",
      "Max profit so far: 1.363601383781054\n",
      "************************** ************* **************************\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9992]])\n",
      "rank, episode_length 1 82000\n",
      "state: tensor([1., 1.])\n",
      "q_a tensor([[-2.0176,  2.9075, -0.7582,  1.4284, -0.6267, -0.0123, -0.0162]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.9751, 0.0249, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0054]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.1167]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [1.00000000e+01 2.88143780e+00 3.83655574e-01 1.19485826e-01\n",
      " 5.26406806e-03 3.02574387e-03 4.43222382e-03 5.37008221e-03\n",
      " 6.89129696e-03 3.53804016e-02]\n",
      "log prob tensor([[-0.0253]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.5000]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.0897]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.0393]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.0700]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[1.0007]])\n",
      "rank, episode_length 1 83000\n",
      "state: tensor([1., 1.])\n",
      "q_a tensor([[-1.4493,  2.2953, -0.7728,  1.2282, -0.5230, -0.0138, -0.0163]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([-1.0000e+20,  1.0000e+00,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.0000, 0.9556, 0.0444, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[6.7077]])\n",
      "mu tensor([[0.0024]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.1818]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [10.          2.19428584  0.30664462  0.25708559  0.05647532  0.01645566\n",
      "  0.02385039  0.02880598  0.05143913  0.20975044]\n",
      "log prob tensor([[-0.0455]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[0.6076]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.6185]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.1172]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.5599]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "value_diff tensor([[0.9992]])\n",
      "rank, episode_length 1 84000\n",
      "state: tensor([1., 0.])\n",
      "q_a tensor([[-2.0047,  2.4275, -0.3682,  1.1840, -0.5099, -0.0140, -0.0130]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "possible action tensor([ 1.0000e+00, -1.0000e+20,  1.0000e+00, -1.0000e+20, -1.0000e+20,\n",
      "        -1.0000e+20, -1.0000e+20])\n",
      "prob_total tensor([[0.1629, 0.0000, 0.8371, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "action_time tensor([[0.]])\n",
      "mu tensor([[0.0088]], grad_fn=<AddmmBackward0>)\n",
      "entropy_total tensor([[0.4445]], grad_fn=<AddBackward0>)\n",
      "entropy_undercut tensor(-0.1905)\n",
      "memPool Canonical [9.24037083e+00 2.02989041e-02 3.44209547e-03 2.37902627e-03\n",
      " 1.35902739e-03 6.19569397e-04 8.24132507e-04 9.61676386e-04\n",
      " 1.07739319e-03 6.32429347e-03]\n",
      "log prob tensor([[-0.1779]], grad_fn=<GatherBackward0>)\n",
      "log prob undercut tensor([[-0.4774]], grad_fn=<LogBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "policy_loss_a tensor([[-0.9256]], grad_fn=<SubBackward0>)\n",
      "value_loss_a tensor([[0.3464]], grad_fn=<AddBackward0>)\n",
      "total loss: tensor([[-0.7524]], grad_fn=<AddBackward0>)\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Code adapted from \"https://github.com/ikostrikov/pytorch-a3c\" and \"https://github.com/dgriff777/a3c_continuous\" \n",
    "import os\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from torch.multiprocessing import Pool\n",
    "import my_optim\n",
    "from model import ActorCritic\n",
    "from test import test\n",
    "from train import train\n",
    "import easydict\n",
    "from mempool_analyzer import Mempool_data\n",
    "from normal_reward import Honest_mining_block_reward\n",
    "from fee_30_20_10_ratio import fee_ratio\n",
    "from lower_bound_profits import lower_bound_selfish_mining_profitability\n",
    "from time_fee_equation import time_fee\n",
    "\n",
    "A3C_args = easydict.EasyDict({\n",
    "        \"lr\": 0.000001,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.99,\n",
    "        \"entropy_coef\": 0.01,\n",
    "        \"value_loss_coef\": 0.5,\n",
    "        \"max_grad_norm\": 50,\n",
    "        \"noise_std\": 0.2,\n",
    "        \"reward_scaling_coefficient\": 0.1,\n",
    "        \"N_nodes_per_layer\": 256,\n",
    "        \"seed\": 1,\n",
    "        \"num_processes_train\": 30,\n",
    "        \"num_processes_test\": 2,\n",
    "        \"num_steps\": 10,\n",
    "        \"testing_episode_length\": 100000,\n",
    "        \"training_print_length\": 1000,\n",
    "        \"max_episode_length\": 1000000,\n",
    "        \"env_name\": 'Blockchain',\n",
    "        \"long_range_testing\": False,\n",
    "        \"testing_index\": 0,\n",
    "        \"no_shared\": False\n",
    "    })\n",
    "\n",
    "BTC_args = easydict.EasyDict({\n",
    "        \"adversarial_ratio\": 0.45,\n",
    "        \"rational_ratio\": 0,\n",
    "        \"connectivity\": 1,\n",
    "        \"max_fork\": 20, \n",
    "        \"len_abort\": 1,\n",
    "        \"mining_time\": 10,\n",
    "        \"block_fixed_reward\": 0, \n",
    "        \"max_block_size\": 1,\n",
    "        \"attack_type\": 1,  # 0: selfish_mining_undercutting, 1: selfish_mining, 2: undercutting\n",
    "        \"noise\": False,\n",
    "        \"epsilon\": 0,\n",
    "        \"Diff_adjusted\": False, # False: before difficulty adjustment, True: after difficulty adjustment\n",
    "        \"state_length\": None,\n",
    "        \"n_action\": None\n",
    "    })\n",
    "\n",
    "# Mempool statistics are obtained by analyzing the data available on https://jochen-hoenicke.de/queue/#BTC,24h,weight\n",
    "# Mempool statistics for range: from 18/12/2023_23_00_00 to 18/12/2023_23_59_59\n",
    "Mempool_args = easydict.EasyDict({\n",
    "        \"start_date_str\": '18/12/2023_23_00_00',\n",
    "        \"end_date_str\": '18/12/2023_23_59_59',\n",
    "        \"N_memPool_section\": 10,\n",
    "        \"sat_per_byte_range_length\": 50,\n",
    "        \"N_steps_honest_mining\": 100000,\n",
    "        \"Base_sat_per_byte_range\": 80,\n",
    "        \"coefficient_mempool_reward\": [[0.14820472025394815, 0.7282113495520344, 5.643656907094102e-13], [0.028176438487433484, 0.9737897315715454, 2.3789063131700735e-15], [0.0031013597177395715, 1.0425169856310426, 0.001258925715929511], [0.0015461140408195586, 1.2052345863085565, 0.00134868764746486], [0.00031051785879065904, 1.6901314778574512, 0.0011832712576748628], [0.0005666599801514915, 1.0342370769473448, 0.0002195612705050585], [0.0009569976216924068, 0.9847738342913699, 0.00013723654148004898], [0.0012181085040403486, 0.9680190302600027, 8.24182077841877e-05], [0.0011097634762145296, 1.2298617957556142, 0.00034394872578617547], [0.006865268297209494, 1.0648883461564944, 0.0015278300081780468]],\n",
    "        \"fee_twenty_ten_ratio\": None,\n",
    "        \"fee_thirty_ten_ratio\": None,\n",
    "        \"normal_adversarial_block_reward_per_min\": None,\n",
    "        \"noise_std_mempool_reward\": None\n",
    "    })\n",
    "\n",
    "# Mempool statistics for range: from 6/9/2024_8_00_00 to 6/9/2024_8_59_59\n",
    "# Mempool_args = easydict.EasyDict({\n",
    "#         \"start_date_str\": '6/9/2024_8_00_00',\n",
    "#         \"end_date_str\": '6/9/2024_8_59_59',\n",
    "#         \"N_memPool_section\": 10,\n",
    "#         \"sat_per_byte_range_length\": 1,\n",
    "#         \"N_steps_honest_mining\": 100000,\n",
    "#         \"coefficient_mempool_reward\": [[0.055806873308782215, 0.38072178794547107, 1.595299419478695e-14], [0.021693107288395685, 0.6800462872806099, 0.04374277956444347], [0.024167414548314777, 0.8044262036320328, 3.7726809116912525e-20], [0.024452552476114324, 0.7831778010712924, 6.142812348160881e-16], [0.0004273199223943512, 1.7661094945688913, 0.004719795490864381], [2.175811997791172e-05, 2.2668297629100285, 0.0045860815832734145], [0.0004564992915334651, 1.3613028638664004, 0.0013761469651945573], [0.0004564992915334651, 1.3613028638664004, 0.0013761469651945573], [4.0618094859605475e-07, 3.4100103962391883, 0.0019188736640224694], [0.0005581315530157322, 1.6358162269055594, 0.012445199605116525]],\n",
    "#         \"Base_sat_per_byte_range\": 2,\n",
    "#         \"fee_twenty_ten_ratio\": None,\n",
    "#         \"fee_thirty_ten_ratio\": None,\n",
    "#         \"normal_adversarial_block_reward_per_min\": None,\n",
    "#         \"noise_std_mempool_reward\": None\n",
    "#     })\n",
    "\n",
    "# Mempool statistics for range: from 15/10/2023_8_00_00 to 15/10/2023_8_59_59\n",
    "# Mempool_args = easydict.EasyDict({\n",
    "#         \"start_date_str\": '15/10/2023_8_00_00',\n",
    "#         \"end_date_str\": '15/10/2023_8_59_59',\n",
    "#         \"N_memPool_section\": 10,\n",
    "#         \"sat_per_byte_range_length\": 1,\n",
    "#         \"N_steps_honest_mining\": 100000,\n",
    "#         \"coefficient_mempool_reward\": [[0.06592696644240723, 0.9213874131703799, 0.05589698973317477], [0.03215894190329676, 0.6237651789702087, 7.77328093837064e-12], [0.01703337274000704, 0.8312690283643408, 2.383520093265834e-16], [0.005023481905600935, 0.8622179583493358, 1.045809783118055e-14], [0.0020465327541133, 0.7346101245070619, 2.0290183142002714e-13], [0.000745904081654545, 0.7860338241302187, 2.332220701952698e-16], [0.0009694665405339612, 0.8846388803371542, 1.9702843186863277e-10], [6.3980591818408935e-06, 2.6370934257885517, 0.001972280378113425], [6.3980591818408935e-06, 2.6370934257885517, 0.001972280378113425], [0.010759845969409917, 1.3062356069124947, 0.012778063200197348]],\n",
    "#         \"Base_sat_per_byte_range\": 1,\n",
    "#         \"fee_twenty_ten_ratio\": None,\n",
    "#         \"fee_thirty_ten_ratio\": None,\n",
    "#         \"normal_adversarial_block_reward_per_min\": None,\n",
    "#         \"noise_std_mempool_reward\": None\n",
    "#     })\n",
    "\n",
    "\n",
    "# def change_parameters():\n",
    "#     global Mempool_args\n",
    "#     Mempool_args.sat_per_byte_range_length = float(input(\"Enter new value for Mempool_args['sat_per_byte_range_length']: \"))\n",
    "\n",
    "def mempool_main():\n",
    "    mempool = Mempool_data(Mempool_args)\n",
    "    coefficient, noise_std, base_sat_per_byte = mempool.extract_mempool_data()\n",
    "    # Prompting the user for input\n",
    "    user_input = input(\"Enter 'c' to continue or 's' to stop and change mempool parameters: \").strip().lower()\n",
    "    if user_input == 's':\n",
    "        change_parameters()\n",
    "        mempool_main()\n",
    "    elif user_input == 'c':\n",
    "        print(\"Continuing with the code...\")\n",
    "    else:\n",
    "        print(\"Invalid input. Exiting the code.\")\n",
    "    return coefficient, noise_std, base_sat_per_byte\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "if __name__ == '__main__':\n",
    "    BTC_args.state_length = 5 + 2 * BTC_args.max_fork + Mempool_args.N_memPool_section + 2 * (BTC_args.max_fork * Mempool_args.N_memPool_section)\n",
    "    BTC_args.n_action = 6 + BTC_args.len_abort\n",
    "    time_fee(BTC_args, Mempool_args)\n",
    "    Mempool_args.normal_adversarial_block_reward_per_min = Honest_mining_block_reward(BTC_args, Mempool_args)\n",
    "    Mempool_args.fee_twenty_ten_ratio, Mempool_args.fee_thirty_ten_ratio = fee_ratio(BTC_args, Mempool_args)\n",
    "    profit_before_difficulty_adjustment = lower_bound_selfish_mining_profitability(BTC_args, Mempool_args)\n",
    "    print(\"A lower bound for selfish mining profitability ratio before difficulty adjustment\", profit_before_difficulty_adjustment / BTC_args.adversarial_ratio)\n",
    "    print(\"A3C trining is starting ...\")\n",
    "    print(\"A3C_args:\", A3C_args)\n",
    "    print(\"BTC_args\", BTC_args)\n",
    "    print(\"Mempool_args\", Mempool_args)\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    torch.manual_seed(A3C_args.seed)\n",
    "    shared_model_a = ActorCritic(A3C_args, BTC_args)\n",
    "    shared_model_a.share_memory()\n",
    "\n",
    "    if A3C_args.no_shared:\n",
    "        optimizer_a = None\n",
    "    else:\n",
    "        optimizer_a = my_optim.SharedAdam(shared_model_a.parameters(), lr=A3C_args.lr)\n",
    "        optimizer_a.share_memory()\n",
    "\n",
    "    processes = []\n",
    "    manager = mp.Manager()\n",
    "    counter = manager.Value('i', 0)\n",
    "    profit_ratio_max = manager.Value('d', 1)\n",
    "    lock = manager.Lock()\n",
    "    N_train_agents = A3C_args.num_processes_train if not A3C_args.long_range_testing else 0\n",
    "    args_list_train = [(rank, A3C_args, BTC_args, Mempool_args, shared_model_a, optimizer_a, counter, profit_ratio_max, lock) for rank in range(N_train_agents)]\n",
    "    args_list_test = [(rank, A3C_args, BTC_args, Mempool_args, shared_model_a, counter, profit_ratio_max) for rank in range(A3C_args.num_processes_test)]\n",
    "    with Pool(A3C_args.num_processes_train + A3C_args.num_processes_test) as pool:\n",
    "        p1, p2 = pool.starmap_async(test, args_list_test), pool.starmap_async(train, args_list_train)\n",
    "        p1.get()\n",
    "        p2.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2a920-0f0b-4d3b-99c2-deda289e150d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a3e85-8197-42b4-a372-812587be4606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A3C",
   "language": "python",
   "name": "a3c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
